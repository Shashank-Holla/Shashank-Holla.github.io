---
layout: post
title: Call for your attention! Do you remember LSTM?
date: 2020-12-03 19:31:00
description: This is my attempt to jot down idea behind Recurrent Neural network variants such as LSTM and GRU and the thought process behind attention mechanism.
redirect: https://dev.to/shashankholla_10/remember-lstm-3mla
tags: Recurrent-neural-network, natural-language-processing
categories: natural-language-processing
---

Redirecting to another page.

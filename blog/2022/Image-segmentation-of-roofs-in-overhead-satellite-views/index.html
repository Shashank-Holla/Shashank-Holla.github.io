<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Image segmentation of roofs in overhead satellite views | Shashank Holla</title> <meta name="author" content="Shashank Holla"> <meta name="description" content="Segmenting the presence of overhead roofs using convolution models"> <meta name="keywords" content="machine learning, computer vision, natural language processing, deep learning"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?b05f9a0b7405d7c8c89c7465593dea81"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?dadeb9c5d1fd12bc8d37475657446863"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?72726b8ee539fbb075705be54a085a54" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/code-solid.svg?1a49ea7a278620a7ddff9ab6b314e4f4"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://shashank-holla.github.io/blog/2022/Image-segmentation-of-roofs-in-overhead-satellite-views/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?ae7cb035be803e483521e52bb723ec55" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Shashank </span>Holla</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Image segmentation of roofs in overhead satellite views</h1> <p class="post-meta">May 5, 2022</p> <p class="post-tags"> <a href="/blog/2022"> <i class="fas fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/image-segmentation"> <i class="fas fa-hashtag fa-sm"></i> Image-Segmentation,</a>   <a href="/blog/tag/convolution"> <i class="fas fa-hashtag fa-sm"></i> Convolution,</a>   <a href="/blog/tag/encoder-decoder"> <i class="fas fa-hashtag fa-sm"></i> encoder-decoder,</a>   <a href="/blog/tag/high-resolution-satellite-images"> <i class="fas fa-hashtag fa-sm"></i> high-resolution-satellite-images</a>     ·   <a href="/blog/category/computer-vision"> <i class="fas fa-tag fa-sm"></i> computer-vision</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="1-what-do-we-do">1. What do we do?</h2> <p>Satellite imagery has been found to be useful in wide variety of applications such as meteorology, conservation of biodiversity and city planning. Satellite images provide clear and accurate representation of the atmospheric motion, geological changes and aids in forecasting and planning.</p> <p>In urban planning and development, satellite images are studied to draw up accurate overhead maps of buildings. The roof segmentation in such images is useful in 3- dimensional reconstruction of cities to identify spots for installation of solar panels.</p> <p>Semantic segmentation of the satellite images for rooftops tries to identify and segment the planar structures in roofs. It involves classifying each pixel into binary class of whether they contain a roof or not. Further, the pixel labels are clustered into segment groups.</p> <p>In this blog, we look at a fully convolutional encoder-decoder based architecture with skip connections that can extract context and provide precise localization for the rooftop segmentation. Further, we explore data augmentation techniques to provide sufficient images for model training and reduce computation constraints.</p> <p><br></p> <h2 id="2-the-data-that-we-use">2. The data that we use</h2> <p>The model training and inferencing has been performed on Inria Aerial Image Labeling Dataset. This is a publicly available dataset which covers urban settlements ranging from densely populated areas to alpine towns. It covers five main regions- Austin, Kitsap, Chicago, Vienna and West Tyrol with 36 image per location. Image resolution of this dataset is 5000*5000 pixels. The dataset is aerial orthorectified color imagery. It also contains the ground truth for two class building and non-building for the training data.</p> <p>Large spatial scale of the satellite images hampers training on available hardware. Also, the small scale of the dataset is not effective for meaningful training. To overcome these challenges, each image is tiled in a non-overlapping manner to provide tiles of 1000 x 1000 pixels. Further, these tiles are random cropped into patch of 256 x 256 pixels at the point of training. Spatial structure of the satellite and mask images are retained during random cropping. Further we have provided option of applying random rotation to increase data augmentation. These have not been applied in the current run. We have also explored the option of resizing the tiles instead. These results are discussed below. After the data augmentation, we have 14400 images for training and 3600 images for testing.</p> <p><br></p> <h2 id="3-designing-the-model">3. Designing the model</h2> <p>Semantic segmentation is a dense prediction task. That is, the prediction’s resolution is same as that of the input. For this purpose, an encoder-decoder architecture has been implemented. The rationale behind the proposed model is that the encoder learns the features of the input image (edges, gradients, parts of the object) and the decoder utilizes these features in providing an accurate segmentation map. The proposed model called SatUMaskNet is based on UNet, a state-of-the-art model for medical image segmentation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/overhead-rooftop/architecture-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/overhead-rooftop/architecture-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/overhead-rooftop/architecture-1400.webp"></source> <img src="/assets/img/overhead-rooftop/architecture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="architecture" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/overhead-rooftop/encoder_decoder-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/overhead-rooftop/encoder_decoder-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/overhead-rooftop/encoder_decoder-1400.webp"></source> <img src="/assets/img/overhead-rooftop/encoder_decoder.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="encoder-decoder" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Proposed convolutional encoder-decoder architecture. </div> <p>The proposed model has 4 blocks each of the encoder, decoder segment. Double convolution block is first applied on the input image before feeding to the encoder block. The double convolution block has two layers each of 3x3 convolution, ReLU activation and batch normalization. The encoder block consists of double convolution block and a skip connection. The skip connection bypasses the 3x3 convolution block and sums with its output. Pointwise convolution is applied on the skipped connection to match the convolution output in the channel dimension. Later, Maxpooling is applied to reduce the spatial dimension of the feature map by half.</p> <p>The decoder block applies up sampling on the feature map. The decoder also takes the encoder output from the same feature level as input. The upsampled output is concatenated with encoder feature map. Further 3x3 double convolution is applied on this output. Decoder block also uses skip connection with pointwise convolution to match the skip connection’s channel dimension with the convolution output. The proposed model is considerably lightweight with 7.5 million parameters.</p> <p><br></p> <h2 id="4-teach-the-model">4. Teach the model</h2> <p>Binary cross entropy loss and dice loss has been used to calculate the overall loss. Binary cross entropy loss provides a large penalty when incorrect predictions are made with high probability. Dice loss is calculated by taking the difference of Dice coefficient from 1. Dice coefficient provides the measure of overlap between the ground truth and the predicted output</p> \[Dice\ Loss=1\ -\ (2\ .\ |X\ \cap\ Y|)/(|X|\ +\ |Y|)\] <p><br></p> <h2 id="5-how-to-train">5. How to train?</h2> <p>The network is trained with (SGD) Stochastic Gradient Descent optimizer with Learning rate of 0.01 and momentum to 0.9. Based on the paper by Leslie Smith, the learning rate of the parameter group is set according to the 1 cycle learning rate policy. This is especially useful in our run as the number of epochs trained is quite low due to resource constraints. The model has been trained for 8 epochs.</p> <p><br></p> <h2 id="6-result">6. Result?</h2> <p>Model training has been performed with UNet and our proposed model SatUMaskNet. We have also trained our proposed model on two augmentation methods –</p> <p>a) with random crop of 256x256 patch size</p> <p>b) with resize of 256x256 patch size.</p> <p>Dice score is used as a performance measure for the comparison.</p> <table> <thead> <tr> <th style="text-align: center">Method</th> <th style="text-align: center">Data augmentation</th> <th style="text-align: center">Dice score</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">UNet</td> <td style="text-align: center">Random crop, 256</td> <td style="text-align: center">0.548</td> </tr> <tr> <td style="text-align: center">SatUMaskNet</td> <td style="text-align: center">Random crop, 256</td> <td style="text-align: center">0.475</td> </tr> <tr> <td style="text-align: center">SatUMaskNet</td> <td style="text-align: center">Resize, 256</td> <td style="text-align: center">0.55</td> </tr> </tbody> </table> <p><br></p> <p>The table above shows the comparison of the two models with different data augmentation techniques. UNet model, with maximum channel size of 1024 and 31 million parameters gave Dice score of 0.548. Our proposed model SatUMaskNet has a maximum channel depth of 512 and 7.5 million parameters gave a Dice score of 0.475. We feel there is potential for further improvement in both the models with better data augmentation analysis. The proposed network was also trained on resized tile images of 256x256 pixel which gave a score 0.55.</p> <p><br></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/overhead-rooftop/results-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/overhead-rooftop/results-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/overhead-rooftop/results-1400.webp"></source> <img src="/assets/img/overhead-rooftop/results.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="results" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><br></p> <p>The image below shows the image segmentation results for our proposed network with resized data augmentation. The predicted mask segments show close resemblance to the ground truth. In some cases, the predicted mask lacks clarity and sharpness in the defined straight edges when compared to the ground truth.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/overhead-rooftop/segmentation-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/overhead-rooftop/segmentation-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/overhead-rooftop/segmentation-1400.webp"></source> <img src="/assets/img/overhead-rooftop/segmentation.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="segmentation" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Image segmentation results with SatUMaskNet on resized images. Top row: tiled, resized satellite images. Middle row: ground truth of the satellite images. Last row: mask segmentation prediction by SatUMaskNet. </div> <p><br></p> <h2 id="7-discussions">7. Discussions</h2> <p>Many of the state-of-the-art papers for image segmentation typically use deep networks and may suffer from vanishing gradients problem without careful hyperparameter tuning. Many of the recent vision-based transformer models would require large scale training data to produce comparable results. In this implementation, we have explored a lightweight fully convolutional network with less parameters. However, there are a few notable observations in our proposed network that need further evaluation. <br></p> <h3 id="71-data-augmentation-analysis">7.1. Data Augmentation Analysis</h3> <p><br></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/overhead-rooftop/missingsegments-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/overhead-rooftop/missingsegments-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/overhead-rooftop/missingsegments-1400.webp"></source> <img src="/assets/img/overhead-rooftop/missingsegments.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="missingsegments" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Training and ground truth patches from the satellite images. Top row: random cropped patch of the satellite image used for training. These patches have vegetation, open fields and do not have rooftops. Bottom row: corresponding ground truth for the patches. This shows the absence of the rooftops in the chosen patch. </div> <p>In this approach, we proposed tiling of satellite image into 1000x1000 pixel tiles and then random cropping 256*256 pixel patch for training. This spatial size of the training patch was taken while keeping the resource constraint into consideration. But during our analysis of the results, we have observed patches for training and inference which did not contain the rooftop segments.</p> <p>Most of the times, image segmentation tasks are high data imbalanced problems. In our case specially, the satellite images are of very high resolution and have minute presence of rooftops. As shown in the above figure, with the chosen patch size, there are many occurrences of such patches for training and inference. This requires further evaluation of the ideal image patch, overlapping area and requirement of resizing to provide data for training. <br></p> <h3 id="72-occurrence-of-false-positive">7.2. Occurrence of False positive</h3> <p><br></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/overhead-rooftop/falsepositive-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/overhead-rooftop/falsepositive-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/overhead-rooftop/falsepositive-1400.webp"></source> <img src="/assets/img/overhead-rooftop/falsepositive.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="falsepositive" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Satellite image, ground truth and prediction patches from model inferencing. Top row: satellite image. Middle row: ground truth of the satellite image patch. Bottom row: predicted mask segment by the model. Rightmost column in this image shows the false positive predicted by the model. Satellite image is of the highway and there is no presence of rooftops as shown in the ground truth image. But the model has falsely predicted rooftops. </div> <p>On review of the model training results, there were observations of false positives in the model’s prediction. As shown in the rightmost column in the above figure, the satellite image patch used doesn’t contain rooftops. The ground truth of the satellite image (shown in the middle row) also shows no presence of rooftops. But the model has falsely predicted rooftops during inference. There are such occurrences that need further analysis. We would like to include this observation as well in our future work. Stronger heuristics along with the proposed deeper network and refined data augmentation may help in solving this error. <br></p> <h2 id="8-finally">8. Finally..</h2> <p>In this approach, we have explored a lightweight fully convolutional encoder-decoder with skipped connections network to solve semantic segmentation of high-resolution satellite images. We have explored data augmentation techniques keeping resource considerations in mind. From model inference and observations, we have identified few key limitations which we have discussed above.</p> <p>With respect to future work, we plan to explore with other convolution techniques such as dilated convolutions, pixel shuffle proposed by Shi and experiment with deeper network. We will further improve and try to identify the optimal data augmentation that can help the model in providing accurate, localized segment maps.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/Seq2SeqLearning/">Convolutional Sequence to Sequence learning - A Closer Look</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/LSTM/">Call for your attention! Do you remember LSTM?</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Shashank Holla. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?14a4be4501338ab8486555cfc6dc0c15"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?07eaa7cf937a07ef341b1faf1776b8cf"></script> <script defer src="/assets/js/common.js?85108440d5ed580d281b2dcc25e2b2d9"></script> <script defer src="/assets/js/copy_code.js?bd78cf329e9ccb6ed226722e0a87ad8e" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>